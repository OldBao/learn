"""
Supervised Fine-Tuning (SFT) å®Œæ•´è®­ç»ƒè„šæœ¬
ä»é¢„è®­ç»ƒæ¨¡å‹åˆ°å¯¹è¯æ¨¡å‹

è¾“å…¥ï¼šé¢„è®­ç»ƒçš„ GPT æ¨¡å‹
è¾“å‡ºï¼šèƒ½éµå¾ªæŒ‡ä»¤çš„å¯¹è¯æ¨¡å‹
"""

import torch
import torch.nn as nn
from torch.nn import functional as F
import tiktoken
import json
import time
import os
import urllib.request
from pathlib import Path

# ============================================================================
# é…ç½®
# ============================================================================

class Config:
    # æ¨¡å‹è·¯å¾„
    pretrained_model_path = 'word_level_gpt.pt'  # ä½ è®­ç»ƒçš„é¢„è®­ç»ƒæ¨¡å‹
    
    # SFT æ•°æ®
    # æˆ‘ä»¬ç”¨ Alpaca æ•°æ®é›†ï¼ˆ52k é«˜è´¨é‡æŒ‡ä»¤å¯¹ï¼‰
    alpaca_url = 'https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json'
    data_path = 'alpaca_data.json'
    
    # Tokenizer
    tokenizer_name = 'gpt2'
    
    # è®­ç»ƒ
    batch_size = 8        # SFT é€šå¸¸ç”¨æ›´å°çš„ batch
    learning_rate = 5e-5  # æ¯”é¢„è®­ç»ƒå° 10x
    max_iters = 3000      # SFT ä¸éœ€è¦å¾ˆå¤šæ­¥
    eval_interval = 300
    eval_iters = 50
    
    # ç‰¹æ®Š tokenï¼ˆç”¨äºæ ‡è®°å¯¹è¯ç»“æ„ï¼‰
    instruction_start = "\n### Instruction:\n"
    input_start = "\n### Input:\n"
    response_start = "\n### Response:\n"
    
    # ç³»ç»Ÿ
    device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'
    save_interval = 1000

config = Config()

print("=" * 80)
print("Supervised Fine-Tuning (SFT) è®­ç»ƒ")
print("=" * 80)
print(f"\nè®¾å¤‡: {config.device}")
print(f"é¢„è®­ç»ƒæ¨¡å‹: {config.pretrained_model_path}")
print(f"æ•°æ®é›†: Alpaca (52k æŒ‡ä»¤å¯¹)")


# ============================================================================
# ä¸‹è½½å’Œå‡†å¤‡æ•°æ®
# ============================================================================

print("\n" + "=" * 80)
print("ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡ SFT æ•°æ®")
print("=" * 80)

# ä¸‹è½½ Alpaca æ•°æ®
if not os.path.exists(config.data_path):
    print(f"\nä¸‹è½½ Alpaca æ•°æ®é›†...")
    try:
        urllib.request.urlretrieve(config.alpaca_url, config.data_path)
        print("âœ“ ä¸‹è½½å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        print("\næ‰‹åŠ¨ä¸‹è½½æ–¹æ³•ï¼š")
        print(f"1. è®¿é—®: {config.alpaca_url}")
        print(f"2. ä¿å­˜ä¸º: {config.data_path}")
        print("3. é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
        exit(1)
else:
    print("âœ“ æ•°æ®å·²å­˜åœ¨")

# åŠ è½½æ•°æ®
print("\nåŠ è½½æ•°æ®...")
with open(config.data_path, 'r', encoding='utf-8') as f:
    alpaca_data = json.load(f)

print(f"âœ“ åŠ è½½äº† {len(alpaca_data)} æ¡è®­ç»ƒæ ·æœ¬")

# æŸ¥çœ‹æ•°æ®æ ¼å¼
print("\næ•°æ®æ ¼å¼ç¤ºä¾‹:")
sample = alpaca_data[0]
print(f"  Instruction: {sample['instruction'][:60]}...")
print(f"  Input: {sample['input'][:60] if sample['input'] else '(empty)'}...")
print(f"  Output: {sample['output'][:60]}...")


# åˆå§‹åŒ– tokenizer
print("\nåˆå§‹åŒ– tokenizer...")
enc = tiktoken.get_encoding(config.tokenizer_name)
print("âœ“ Tokenizer åŠ è½½æˆåŠŸ")


# ============================================================================
# æ•°æ®å¤„ç†ï¼šå…³é”®ï¼
# ============================================================================

print("\n" + "=" * 80)
print("ç¬¬äºŒæ­¥ï¼šæ•°æ®å¤„ç†ï¼ˆå…³é”®æ­¥éª¤ï¼‰")
print("=" * 80)

print("""
SFT çš„æ ¸å¿ƒï¼šåªåœ¨"å›ç­”"éƒ¨åˆ†è®¡ç®—æŸå¤±ï¼

æ ¼å¼åŒ–å¯¹è¯ï¼š
  ### Instruction:
  [ç”¨æˆ·æŒ‡ä»¤]
  
  ### Input:
  [å¯é€‰çš„è¾“å…¥]
  
  ### Response:
  [æ¨¡å‹å›ç­”]  â† åªåœ¨è¿™éƒ¨åˆ†è®¡ç®—æŸå¤±ï¼
""")


def format_instruction(sample):
    """
    å°† Alpaca æ ·æœ¬æ ¼å¼åŒ–ä¸ºè®­ç»ƒæ–‡æœ¬
    """
    instruction = sample['instruction']
    input_text = sample['input']
    output = sample['output']
    
    # æ„å»ºå®Œæ•´å¯¹è¯
    if input_text:
        prompt = (
            f"{config.instruction_start}{instruction}"
            f"{config.input_start}{input_text}"
            f"{config.response_start}{output}"
        )
    else:
        prompt = (
            f"{config.instruction_start}{instruction}"
            f"{config.response_start}{output}"
        )
    
    return prompt


def prepare_sft_sample(sample, enc, max_length=512):
    """
    å‡†å¤‡ SFT è®­ç»ƒæ ·æœ¬
    
    å…³é”®ï¼šè¿”å› input_ids å’Œ labels
    labels ä¸­éå›ç­”éƒ¨åˆ†æ ‡è®°ä¸º -100ï¼ˆæŸå¤±å‡½æ•°ä¼šå¿½ç•¥ï¼‰
    """
    # æ ¼å¼åŒ–æ–‡æœ¬
    full_text = format_instruction(sample)
    
    # ç¼–ç 
    tokens = enc.encode(full_text)
    
    # æˆªæ–­ï¼ˆå¦‚æœå¤ªé•¿ï¼‰
    if len(tokens) > max_length:
        tokens = tokens[:max_length]
    
    # æ‰¾åˆ° Response çš„èµ·å§‹ä½ç½®
    response_start_text = config.response_start
    response_start_tokens = enc.encode(response_start_text)
    
    # åœ¨ tokens ä¸­æ‰¾åˆ° response_start çš„ä½ç½®
    response_start_idx = None
    for i in range(len(tokens) - len(response_start_tokens) + 1):
        if tokens[i:i+len(response_start_tokens)] == response_start_tokens:
            response_start_idx = i + len(response_start_tokens)
            break
    
    if response_start_idx is None:
        # å¦‚æœæ‰¾ä¸åˆ°ï¼ˆæ•°æ®æ ¼å¼é—®é¢˜ï¼‰ï¼Œè·³è¿‡è¿™ä¸ªæ ·æœ¬
        return None
    
    # åˆ›å»º labels
    labels = tokens.copy()
    
    # å…³é”®ï¼šResponse ä¹‹å‰çš„éƒ¨åˆ†éƒ½æ ‡è®°ä¸º -100
    for i in range(response_start_idx):
        labels[i] = -100
    
    return {
        'input_ids': tokens,
        'labels': labels,
        'length': len(tokens)
    }


# å¤„ç†æ‰€æœ‰æ•°æ®
print("\nå¤„ç†è®­ç»ƒæ•°æ®...")
processed_data = []

for i, sample in enumerate(alpaca_data):
    processed = prepare_sft_sample(sample, enc)
    if processed is not None:
        processed_data.append(processed)
    
    if (i + 1) % 10000 == 0:
        print(f"  å¤„ç†äº† {i+1}/{len(alpaca_data)} ä¸ªæ ·æœ¬...")

print(f"âœ“ æˆåŠŸå¤„ç† {len(processed_data)} ä¸ªæ ·æœ¬")

# æ˜¾ç¤ºå¤„ç†åçš„æ ·æœ¬
print("\nå¤„ç†åçš„æ ·æœ¬ç¤ºä¾‹:")
sample = processed_data[0]
print(f"  Input IDs é•¿åº¦: {len(sample['input_ids'])}")
print(f"  Labels é•¿åº¦: {len(sample['labels'])}")
print(f"\n  å®Œæ•´æ–‡æœ¬:")
print(f"  {enc.decode(sample['input_ids'])}")
print(f"\n  Labels ä¸­ -100 çš„ä½ç½®ï¼ˆè¿™äº›ä½ç½®ä¸è®¡ç®—æŸå¤±ï¼‰:")
mask_positions = [i for i, label in enumerate(sample['labels']) if label == -100]
print(f"  å‰ 20 ä¸ªä½ç½®çš„ labels: {sample['labels'][:20]}")
print(f"  æ€»å…± {len(mask_positions)} ä¸ªä½ç½®è¢« maskï¼ˆä¸è®¡ç®—æŸå¤±ï¼‰")

# åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
train_size = int(0.95 * len(processed_data))
train_data = processed_data[:train_size]
val_data = processed_data[train_size:]

print(f"\næ•°æ®åˆ’åˆ†:")
print(f"  è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
print(f"  éªŒè¯é›†: {len(val_data)} æ ·æœ¬")


# ============================================================================
# æ•°æ®åŠ è½½å™¨
# ============================================================================

def get_batch(split, batch_size=config.batch_size):
    """
    è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„ SFT æ•°æ®
    """
    data = train_data if split == 'train' else val_data
    
    # éšæœºé€‰æ‹©æ ·æœ¬
    indices = torch.randint(len(data), (batch_size,))
    
    # æ‰¾åˆ°æœ€å¤§é•¿åº¦ï¼ˆç”¨äº paddingï¼‰
    max_len = max(data[i]['length'] for i in indices)
    
    # å‡†å¤‡ batch
    input_ids = []
    labels = []
    
    for idx in indices:
        sample = data[idx]
        
        # Padding
        pad_len = max_len - sample['length']
        
        input_id = sample['input_ids'] + [enc.eot_token] * pad_len
        label = sample['labels'] + [-100] * pad_len  # padding ä¹Ÿä¸è®¡ç®—æŸå¤±
        
        input_ids.append(input_id)
        labels.append(label)
    
    input_ids = torch.tensor(input_ids, dtype=torch.long, device=config.device)
    labels = torch.tensor(labels, dtype=torch.long, device=config.device)
    
    return input_ids, labels


# æµ‹è¯•æ•°æ®åŠ è½½
print("\næµ‹è¯•æ•°æ®åŠ è½½...")
xb, yb = get_batch('train', batch_size=2)
print(f"  Batch input_ids shape: {xb.shape}")
print(f"  Batch labels shape: {yb.shape}")
print(f"  ç¤ºä¾‹ï¼šç¬¬ä¸€ä¸ªæ ·æœ¬çš„å‰ 20 ä¸ª labels: {yb[0][:20].tolist()}")
print(f"  â†’ æ³¨æ„ -100 çš„ä½ç½®ï¼ˆè¿™äº›æ˜¯æŒ‡ä»¤éƒ¨åˆ†ï¼Œä¸è®¡ç®—æŸå¤±ï¼‰")


# ============================================================================
# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
# ============================================================================

print("\n" + "=" * 80)
print("ç¬¬ä¸‰æ­¥ï¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹")
print("=" * 80)

try:
    print(f"\nåŠ è½½æ¨¡å‹ä» {config.pretrained_model_path}...")
    checkpoint = torch.load(config.pretrained_model_path, map_location=config.device)
    
    # é‡æ–°å¯¼å…¥æ¨¡å‹å®šä¹‰ï¼ˆéœ€è¦å’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    from train_word_level_gpt import GPTLanguageModel, Config as PretrainConfig
    
    pretrain_config = checkpoint['config']
    
    # åˆ›å»ºæ¨¡å‹
    model = GPTLanguageModel()
    model.load_state_dict(checkpoint['model'])
    model = model.to(config.device)
    model.train()
    
    print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    print(f"\næ¨¡å‹å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")
    
except FileNotFoundError:
    print(f"âŒ æ‰¾ä¸åˆ°é¢„è®­ç»ƒæ¨¡å‹: {config.pretrained_model_path}")
    print("\nè¯·å…ˆè¿è¡Œ train_word_level_gpt.py å®Œæˆé¢„è®­ç»ƒ")
    exit(1)
except Exception as e:
    print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
    exit(1)


# ============================================================================
# SFT è®­ç»ƒ
# ============================================================================

print("\n" + "=" * 80)
print("ç¬¬å››æ­¥ï¼šSFT è®­ç»ƒ")
print("=" * 80)

# ä¼˜åŒ–å™¨ï¼ˆå­¦ä¹ ç‡æ¯”é¢„è®­ç»ƒå°ï¼‰
optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)

print(f"\nè®­ç»ƒé…ç½®:")
print(f"  å­¦ä¹ ç‡: {config.learning_rate} (æ¯”é¢„è®­ç»ƒå° 5-10x)")
print(f"  æ‰¹å¤§å°: {config.batch_size}")
print(f"  è®­ç»ƒæ­¥æ•°: {config.max_iters}")
print(f"  è¯„ä¼°é—´éš”: {config.eval_interval}")


@torch.no_grad()
def estimate_loss():
    """ä¼°è®¡è®­ç»ƒ/éªŒè¯æŸå¤±"""
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(config.eval_iters)
        for k in range(config.eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out


# è®­ç»ƒå¾ªç¯
print("\nå¼€å§‹ SFT è®­ç»ƒ...")
start_time = time.time()

train_losses = []
val_losses = []

for iter in range(config.max_iters):
    
    # è¯„ä¼°
    if iter % config.eval_interval == 0 or iter == config.max_iters - 1:
        losses = estimate_loss()
        elapsed = time.time() - start_time
        
        print(f"step {iter:4d} | train {losses['train']:.4f} | val {losses['val']:.4f} | time {elapsed:.1f}s")
        
        train_losses.append(losses['train'])
        val_losses.append(losses['val'])
    
    # è·å–æ‰¹æ¬¡
    xb, yb = get_batch('train')
    
    # å‰å‘ä¼ æ’­ï¼ˆå…³é”®ï¼šlabels ä¸­ -100 çš„ä½ç½®ä¼šè¢«è‡ªåŠ¨å¿½ç•¥ï¼‰
    logits, loss = model(xb, yb)
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    
    # æ¢¯åº¦è£å‰ª
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    
    optimizer.step()
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    if (iter + 1) % config.save_interval == 0:
        checkpoint_path = f'sft_checkpoint_{iter+1}.pt'
        torch.save({
            'model': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'iter': iter,
            'config': pretrain_config,
        }, checkpoint_path)
        print(f"  â†’ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")

total_time = time.time() - start_time
print(f"\nâœ“ SFT è®­ç»ƒå®Œæˆï¼æ€»æ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")


# ============================================================================
# æµ‹è¯•å¯¹è¯èƒ½åŠ›
# ============================================================================

print("\n" + "=" * 80)
print("ç¬¬äº”æ­¥ï¼šæµ‹è¯•å¯¹è¯èƒ½åŠ›")
print("=" * 80)

model.eval()

def generate_response(instruction, input_text="", max_new_tokens=150, temperature=0.7):
    """
    ç”Ÿæˆå›ç­”
    """
    # æ„å»º prompt
    if input_text:
        prompt = (
            f"{config.instruction_start}{instruction}"
            f"{config.input_start}{input_text}"
            f"{config.response_start}"
        )
    else:
        prompt = (
            f"{config.instruction_start}{instruction}"
            f"{config.response_start}"
        )
    
    # ç¼–ç 
    tokens = enc.encode(prompt)
    context = torch.tensor(tokens, dtype=torch.long, device=config.device).unsqueeze(0)
    
    # ç”Ÿæˆ
    generated = model.generate(context, max_new_tokens=max_new_tokens, temperature=temperature, top_k=50)
    
    # è§£ç 
    full_text = enc.decode(generated[0].tolist())
    
    # æå–å›ç­”éƒ¨åˆ†
    response_start = full_text.find(config.response_start)
    if response_start != -1:
        response = full_text[response_start + len(config.response_start):].strip()
    else:
        response = full_text
    
    return response


# æµ‹è¯•å‡ ä¸ªæŒ‡ä»¤
test_instructions = [
    {
        "instruction": "What is the capital of France?",
        "input": ""
    },
    {
        "instruction": "Write a haiku about spring.",
        "input": ""
    },
    {
        "instruction": "Explain what photosynthesis is to a 5-year-old.",
        "input": ""
    },
    {
        "instruction": "Summarize the following text.",
        "input": "The quick brown fox jumps over the lazy dog. This sentence contains every letter of the English alphabet."
    }
]

print("\nç”Ÿæˆç¤ºä¾‹ï¼ˆæ¸©åº¦ 0.7ï¼‰:")
for i, test in enumerate(test_instructions):
    print(f"\n{'='*70}")
    print(f"ç¤ºä¾‹ {i+1}")
    print(f"{'='*70}")
    print(f"Instruction: {test['instruction']}")
    if test['input']:
        print(f"Input: {test['input']}")
    print(f"\nResponse:")
    response = generate_response(test['instruction'], test['input'], temperature=0.7)
    print(response)


# ============================================================================
# ä¿å­˜æœ€ç»ˆæ¨¡å‹
# ============================================================================

print("\n" + "=" * 80)
print("ç¬¬å…­æ­¥ï¼šä¿å­˜æ¨¡å‹")
print("=" * 80)

final_checkpoint = {
    'model': model.state_dict(),
    'optimizer': optimizer.state_dict(),
    'config': pretrain_config,
    'train_losses': train_losses,
    'val_losses': val_losses,
    'sft_config': config,
}

torch.save(final_checkpoint, 'sft_model.pt')
print("âœ“ SFT æ¨¡å‹å·²ä¿å­˜åˆ° sft_model.pt")


# ============================================================================
# å¯¹æ¯”é¢„è®­ç»ƒ vs SFT
# ============================================================================

print("\n" + "=" * 80)
print("ç¬¬ä¸ƒæ­¥ï¼šå¯¹æ¯”é¢„è®­ç»ƒ vs SFT")
print("=" * 80)

print("""
é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä½ è®­ç»ƒçš„ï¼‰:
  ä»»åŠ¡: Next Token Prediction
  è¡Œä¸º: ç»­å†™æ–‡æœ¬
  
  è¾“å…¥: "What is the capital"
  è¾“å‡º: "of France? What is the capital of Germany?..."
        â†‘ åªæ˜¯ç»­å†™ï¼Œä¸å›ç­”é—®é¢˜

SFT æ¨¡å‹ï¼ˆåˆšè®­ç»ƒçš„ï¼‰:
  ä»»åŠ¡: Instruction Following
  è¡Œä¸º: å›ç­”é—®é¢˜
  
  è¾“å…¥: "What is the capital of France?"
  è¾“å‡º: "The capital of France is Paris."
        â†‘ çœŸæ­£å›ç­”äº†é—®é¢˜ï¼

è¿™å°±æ˜¯ SFT çš„é­”åŠ›ï¼
""")


# ============================================================================
# æ€»ç»“
# ============================================================================

print("\n" + "=" * 80)
print("æ€»ç»“")
print("=" * 80)

print(f"""
SFT è®­ç»ƒå®Œæˆï¼

é…ç½®:
  æ•°æ®: Alpaca ({len(train_data)} è®­ç»ƒæ ·æœ¬)
  è®­ç»ƒæ­¥æ•°: {config.max_iters}
  å­¦ä¹ ç‡: {config.learning_rate}
  è®­ç»ƒæ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ

æœ€ç»ˆæ€§èƒ½:
  è®­ç»ƒé›† Loss: {train_losses[-1]:.4f}
  éªŒè¯é›† Loss: {val_losses[-1]:.4f}

å…³é”®æ”¹è¿›:
  âœ“ æ¨¡å‹å­¦ä¼šäº†å¯¹è¯æ ¼å¼
  âœ“ èƒ½éµå¾ªæŒ‡ä»¤
  âœ“ å›ç­”æ›´æœ‰é’ˆå¯¹æ€§

ä¸‹ä¸€æ­¥:
  1. å¤šæµ‹è¯•ä¸åŒçš„æŒ‡ä»¤ï¼Œè¯„ä¼°è´¨é‡
  2. å¦‚æœæ•ˆæœä¸ç†æƒ³ï¼Œå¯ä»¥:
     - è®­ç»ƒæ›´å¤š epochs
     - è°ƒæ•´å­¦ä¹ ç‡
     - ä½¿ç”¨æ›´å¤š/æ›´å¥½çš„æ•°æ®
  3. è¿›é˜¶: å°è¯• DPO è¿›ä¸€æ­¥ä¼˜åŒ–

æ–‡ä»¶:
  - sft_model.pt: æœ€ç»ˆæ¨¡å‹
  - sft_checkpoint_*.pt: ä¸­é—´æ£€æŸ¥ç‚¹
""")

print("\n" + "=" * 80)
print("ğŸ‰ æ­å–œï¼ä½ å·²ç»è®­ç»ƒå‡ºä¸€ä¸ªå¯¹è¯æ¨¡å‹ï¼")
print("=" * 80)