"""
è¯çº§ Mini-GPT è®­ç»ƒè„šæœ¬
ä½¿ç”¨ tiktoken (GPT-2 tokenizer)

ç›¸æ¯”å­—ç¬¦çº§çš„æ”¹è¿›ï¼š
1. æ›´é«˜æ•ˆçš„ç¼–ç ï¼ˆ1ä¸ªå•è¯ â‰ˆ 1ä¸ªtokenï¼Œè€Œä¸æ˜¯å¤šä¸ªå­—ç¬¦ï¼‰
2. æ›´å¤§çš„æœ‰æ•ˆä¸Šä¸‹æ–‡ï¼ˆ256 tokens â‰ˆ 150-200 ä¸ªå•è¯ï¼‰
3. æ›´å¥½çš„è¯­ä¹‰ç†è§£
4. åŠ å…¥äº†å­¦ä¹ ç‡è°ƒåº¦ã€æ¢¯åº¦è£å‰ªç­‰ä¼˜åŒ–
"""

import torch
import torch.nn as nn
from torch.nn import functional as F
import tiktoken
import math
import time
import os
import urllib.request

# ============================================================================
# é…ç½®
# ============================================================================

class Config:
    # æ•°æ®
    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'
    
    # Tokenizer
    tokenizer_name = 'gpt2'  # ä½¿ç”¨ GPT-2 çš„ tokenizer
    
    # æ¨¡å‹
    vocab_size = 50257  # GPT-2 tokenizer çš„è¯è¡¨å¤§å°
    n_layer = 8         # å¢åŠ å±‚æ•°ï¼ˆä» 6 åˆ° 8ï¼‰
    n_head = 8          # å¢åŠ æ³¨æ„åŠ›å¤´ï¼ˆä» 6 åˆ° 8ï¼‰
    n_embd = 512        # å¢åŠ ç»´åº¦ï¼ˆä» 384 åˆ° 512ï¼‰
    dropout = 0.2
    block_size = 256    # ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆç°åœ¨æ˜¯ 256 ä¸ªè¯ï¼Œè€Œä¸æ˜¯ 256 ä¸ªå­—ç¬¦ï¼ï¼‰
    
    # è®­ç»ƒ
    batch_size = 32     # å‡å° batchï¼ˆå› ä¸ºæ¨¡å‹æ›´å¤§äº†ï¼‰
    learning_rate = 3e-4
    max_iters = 10000   # å¢åŠ è®­ç»ƒæ­¥æ•°
    eval_interval = 500
    eval_iters = 100
    
    # å­¦ä¹ ç‡è°ƒåº¦
    warmup_iters = 500
    min_lr = 3e-5
    
    # ç³»ç»Ÿ
    device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'
    compile_model = False  # PyTorch 2.0+ å¯ä»¥å¼€å¯

config = Config()

print("=" * 80)
print("è¯çº§ Mini-GPT è®­ç»ƒ")
print("=" * 80)
print(f"\nè®¾å¤‡: {config.device}")
print(f"Tokenizer: {config.tokenizer_name}")
print(f"è¯è¡¨å¤§å°: {config.vocab_size:,}")
print(f"ä¸Šä¸‹æ–‡é•¿åº¦: {config.block_size} tokens")
print(f"æ¨¡å‹ç»´åº¦: {config.n_embd}")
print(f"å±‚æ•°: {config.n_layer}")
print(f"æ³¨æ„åŠ›å¤´æ•°: {config.n_head}")


# ============================================================================
# æ•°æ®å‡†å¤‡
# ============================================================================

print("\n" + "=" * 80)
print("ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡æ•°æ®å’Œ Tokenizer")
print("=" * 80)

# ä¸‹è½½æ•°æ®
data_path = 'shakespeare.txt'
if not os.path.exists(data_path):
    print(f"ä¸‹è½½æ•°æ®ä» {config.data_url}...")
    urllib.request.urlretrieve(config.data_url, data_path)
    print("ä¸‹è½½å®Œæˆï¼")
else:
    print("æ•°æ®å·²å­˜åœ¨")

# è¯»å–æ•°æ®
with open(data_path, 'r', encoding='utf-8') as f:
    text = f.read()

print(f"\næ•°æ®ç»Ÿè®¡:")
print(f"  æ€»å­—ç¬¦æ•°: {len(text):,}")

# åˆå§‹åŒ– tiktoken tokenizer
print(f"\nåŠ è½½ tiktoken tokenizer: {config.tokenizer_name}")
try:
    enc = tiktoken.get_encoding(config.tokenizer_name)
    print("âœ“ Tokenizer åŠ è½½æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ åŠ è½½å¤±è´¥: {e}")
    print("å°è¯•å®‰è£…: pip install tiktoken")
    exit(1)

# ç¼–ç æ•´ä¸ªæ–‡æœ¬
print("\nç¼–ç æ–‡æœ¬...")
tokens = enc.encode(text)
print(f"âœ“ ç¼–ç å®Œæˆï¼")
print(f"  Token æ•°é‡: {len(tokens):,}")
print(f"  å‹ç¼©æ¯”: {len(text)/len(tokens):.2f} å­—ç¬¦/token")

# æµ‹è¯•ç¼–ç /è§£ç 
test_text = "Hello, world! How are you?"
test_tokens = enc.encode(test_text)
test_decoded = enc.decode(test_tokens)

print(f"\nç¼–ç æµ‹è¯•:")
print(f"  åŸæ–‡: {test_text}")
print(f"  Tokens: {test_tokens}")
print(f"  Token æ•°: {len(test_tokens)}")
print(f"  è§£ç : {test_decoded}")

# æ˜¾ç¤ºä¸€äº› token çš„æ–‡æœ¬
print(f"\nå‰ 20 ä¸ª tokens å¯¹åº”çš„æ–‡æœ¬:")
for i, token in enumerate(tokens[:20]):
    token_text = enc.decode([token])
    # å¤„ç†ç‰¹æ®Šå­—ç¬¦æ˜¾ç¤º
    if token_text == '\n':
        token_text = '\\n'
    elif token_text == ' ':
        token_text = 'Â·'  # ç”¨ä¸­ç‚¹è¡¨ç¤ºç©ºæ ¼
    print(f"  {i:2d}. {token:5d} â†’ '{token_text}'")

# è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†
data = torch.tensor(tokens, dtype=torch.long)
n = int(0.9 * len(data))
train_data = data[:n]
val_data = data[n:]

print(f"\næ•°æ®åˆ’åˆ†:")
print(f"  è®­ç»ƒé›†: {len(train_data):,} tokens")
print(f"  éªŒè¯é›†: {len(val_data):,} tokens")


# æ•°æ®åŠ è½½å™¨
def get_batch(split):
    """è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - config.block_size, (config.batch_size,))
    x = torch.stack([data[i:i+config.block_size] for i in ix])
    y = torch.stack([data[i+1:i+config.block_size+1] for i in ix])
    x, y = x.to(config.device), y.to(config.device)
    return x, y


# æµ‹è¯•æ•°æ®åŠ è½½
xb, yb = get_batch('train')
print(f"\næ‰¹æ¬¡æ•°æ®:")
print(f"  è¾“å…¥å½¢çŠ¶: {xb.shape}")
print(f"  æ ‡ç­¾å½¢çŠ¶: {yb.shape}")
print(f"  ç¬¬ä¸€ä¸ªæ ·æœ¬çš„å‰10ä¸ªtoken:")
print(f"    è¾“å…¥ tokens: {xb[0][:10].tolist()}")
print(f"    æ ‡ç­¾ tokens: {yb[0][:10].tolist()}")
print(f"  è§£ç å:")
print(f"    è¾“å…¥: {enc.decode(xb[0][:10].tolist())}")
print(f"    æ ‡ç­¾: {enc.decode(yb[0][:10].tolist())}")


# ============================================================================
# æ¨¡å‹å®šä¹‰ï¼ˆå’Œå­—ç¬¦çº§ç›¸åŒï¼Œä½†å‚æ•°æ›´å¤§ï¼‰
# ============================================================================

print("\n" + "=" * 80)
print("ç¬¬äºŒæ­¥ï¼šå®šä¹‰æ¨¡å‹")
print("=" * 80)

class Head(nn.Module):
    """å•ä¸ªæ³¨æ„åŠ›å¤´"""
    
    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(config.n_embd, head_size, bias=False)
        self.query = nn.Linear(config.n_embd, head_size, bias=False)
        self.value = nn.Linear(config.n_embd, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, x):
        B, T, C = x.shape
        k = self.key(x)
        q = self.query(x)
        
        wei = q @ k.transpose(-2, -1) * (C ** -0.5)
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
        wei = F.softmax(wei, dim=-1)
        wei = self.dropout(wei)
        
        v = self.value(x)
        out = wei @ v
        return out


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›"""
    
    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        self.proj = nn.Linear(config.n_embd, config.n_embd)
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.proj(out))
        return out


class FeedForward(nn.Module):
    """å‰é¦ˆç½‘ç»œ"""
    
    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(config.dropout),
        )
    
    def forward(self, x):
        return self.net(x)


class Block(nn.Module):
    """Transformer å—ï¼ˆPre-LNï¼‰"""
    
    def __init__(self, n_embd, n_head):
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_head, head_size)
        self.ffwd = FeedForward(n_embd)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)
    
    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        return x


class GPTLanguageModel(nn.Module):
    """GPT è¯­è¨€æ¨¡å‹"""
    
    def __init__(self):
        super().__init__()
        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)
        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)
        self.blocks = nn.Sequential(*[Block(config.n_embd, config.n_head) for _ in range(config.n_layer)])
        self.ln_f = nn.LayerNorm(config.n_embd)
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)
        
        # æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
    
    def forward(self, idx, targets=None):
        B, T = idx.shape
        device = idx.device
        
        tok_emb = self.token_embedding_table(idx)
        pos_emb = self.position_embedding_table(torch.arange(T, device=device))
        x = tok_emb + pos_emb
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)
        
        return logits, loss
    
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        """ç”Ÿæˆæ–‡æœ¬ï¼ˆå¸¦ temperature å’Œ top-kï¼‰"""
        for _ in range(max_new_tokens):
            idx_cond = idx[:, -config.block_size:]
            logits, loss = self(idx_cond)
            logits = logits[:, -1, :] / temperature
            
            # Top-k é‡‡æ ·
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')
            
            probs = F.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), dim=1)
        return idx


# åˆ›å»ºæ¨¡å‹
model = GPTLanguageModel()
model = model.to(config.device)

# ç»Ÿè®¡å‚æ•°
total_params = sum(p.numel() for p in model.parameters())
print(f"\næ¨¡å‹å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")

# å¯é€‰ï¼šç¼–è¯‘æ¨¡å‹ï¼ˆPyTorch 2.0+ï¼‰
if config.compile_model and hasattr(torch, 'compile'):
    print("ç¼–è¯‘æ¨¡å‹...")
    model = torch.compile(model)

# æµ‹è¯•å‰å‘ä¼ æ’­
xb, yb = get_batch('train')
logits, loss = model(xb, yb)
print(f"\nå‰å‘ä¼ æ’­æµ‹è¯•:")
print(f"  Logits å½¢çŠ¶: {logits.shape}")
print(f"  åˆå§‹ Loss: {loss.item():.4f}")
print(f"  é¢„æœŸåˆå§‹ Loss: {math.log(config.vocab_size):.4f} (éšæœºçŒœæµ‹)")


# ============================================================================
# è®­ç»ƒ
# ============================================================================

print("\n" + "=" * 80)
print("ç¬¬ä¸‰æ­¥ï¼šå¼€å§‹è®­ç»ƒ")
print("=" * 80)

# ä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)


# å­¦ä¹ ç‡è°ƒåº¦
def get_lr(iter):
    """Warmup + Cosine Decay"""
    # Warmup
    if iter < config.warmup_iters:
        return config.learning_rate * iter / config.warmup_iters
    # å·²å®Œæˆ
    if iter > config.max_iters:
        return config.min_lr
    # Cosine decay
    decay_ratio = (iter - config.warmup_iters) / (config.max_iters - config.warmup_iters)
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return config.min_lr + coeff * (config.learning_rate - config.min_lr)


@torch.no_grad()
def estimate_loss():
    """ä¼°è®¡è®­ç»ƒ/éªŒè¯æŸå¤±"""
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(config.eval_iters)
        for k in range(config.eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out


# è®­ç»ƒå¾ªç¯
print("\nå¼€å§‹è®­ç»ƒ...")
print(f"é…ç½®: {config.max_iters} æ­¥, batch_size={config.batch_size}, lr={config.learning_rate}")
print(f"å­¦ä¹ ç‡è°ƒåº¦: warmup={config.warmup_iters}, min_lr={config.min_lr}")
start_time = time.time()

train_losses = []
val_losses = []
lrs = []

for iter in range(config.max_iters):
    
    # è¯„ä¼°
    if iter % config.eval_interval == 0 or iter == config.max_iters - 1:
        losses = estimate_loss()
        elapsed = time.time() - start_time
        lr = get_lr(iter)
        
        print(f"step {iter:5d} | train {losses['train']:.4f} | val {losses['val']:.4f} | lr {lr:.2e} | time {elapsed:.1f}s")
        
        train_losses.append(losses['train'])
        val_losses.append(losses['val'])
        lrs.append(lr)
    
    # æ›´æ–°å­¦ä¹ ç‡
    lr = get_lr(iter)
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    
    # è·å–æ‰¹æ¬¡
    xb, yb = get_batch('train')
    
    # å‰å‘ä¼ æ’­
    logits, loss = model(xb, yb)
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    
    # æ¢¯åº¦è£å‰ª
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    
    optimizer.step()

total_time = time.time() - start_time
print(f"\nè®­ç»ƒå®Œæˆï¼æ€»æ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")


# ============================================================================
# ç”Ÿæˆæ–‡æœ¬
# ============================================================================

print("\n" + "=" * 80)
print("ç¬¬å››æ­¥ï¼šç”Ÿæˆæ–‡æœ¬")
print("=" * 80)

model.eval()

def generate_text(prompt, max_new_tokens=200, temperature=0.8, top_k=10):
    """ç”Ÿæˆæ–‡æœ¬çš„è¾…åŠ©å‡½æ•°"""
    tokens = enc.encode(prompt)
    context = torch.tensor(tokens, dtype=torch.long, device=config.device).unsqueeze(0)
    generated = model.generate(context, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)
    return enc.decode(generated[0].tolist())


# æµ‹è¯•ä¸åŒçš„æç¤º
prompts = [
    "ROMEO:",
    "To be or not to be",
    "First Citizen:\n",
    "JULIET:\n"
]

print("\nç”Ÿæˆç¤ºä¾‹ï¼ˆtemperature=0.8, top_k=10ï¼‰:")
for prompt in prompts:
    print(f"\n{'='*70}")
    print(f"æç¤º: '{prompt}'")
    print(f"{'='*70}")
    text = generate_text(prompt, max_new_tokens=150, temperature=0.8, top_k=10)
    print(text)


# å¯¹æ¯”ä¸åŒçš„ temperature
print("\n\n" + "=" * 80)
print("Temperature å¯¹æ¯”")
print("=" * 80)

prompt = "ROMEO:"
temps = [0.5, 0.8, 1.0, 1.2]

for temp in temps:
    print(f"\n{'='*70}")
    print(f"Temperature = {temp}")
    print(f"{'='*70}")
    text = generate_text(prompt, max_new_tokens=100, temperature=temp, top_k=10)
    print(text)


# ============================================================================
# ä¿å­˜æ¨¡å‹
# ============================================================================

print("\n" + "=" * 80)
print("ç¬¬äº”æ­¥ï¼šä¿å­˜æ¨¡å‹")
print("=" * 80)

checkpoint = {
    'model': model.state_dict(),
    'optimizer': optimizer.state_dict(),
    'config': config,
    'train_losses': train_losses,
    'val_losses': val_losses,
    'lrs': lrs,
    'tokenizer': config.tokenizer_name,
}

torch.save(checkpoint, 'word_level_gpt.pt')
print("âœ“ æ¨¡å‹å·²ä¿å­˜åˆ° word_level_gpt.pt")


# ============================================================================
# å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
# ============================================================================

print("\n" + "=" * 80)
print("ç¬¬å…­æ­¥ï¼šå¯è§†åŒ–è®­ç»ƒ")
print("=" * 80)

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

# Loss æ›²çº¿
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
steps = [i * config.eval_interval for i in range(len(train_losses))]
plt.plot(steps, train_losses, label='Train Loss', linewidth=2)
plt.plot(steps, val_losses, label='Val Loss', linewidth=2)
plt.xlabel('Training Steps')
plt.ylabel('Loss')
plt.title('Training Progress')
plt.legend()
plt.grid(True, alpha=0.3)

# å­¦ä¹ ç‡æ›²çº¿
plt.subplot(1, 2, 2)
plt.plot(steps, lrs, linewidth=2, color='green')
plt.xlabel('Training Steps')
plt.ylabel('Learning Rate')
plt.title('Learning Rate Schedule')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_curves.png', dpi=150)
print("âœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° training_curves.png")


# ============================================================================
# æ€»ç»“
# ============================================================================

print("\n" + "=" * 80)
print("è®­ç»ƒæ€»ç»“")
print("=" * 80)

print(f"\næ¨¡å‹é…ç½®:")
print(f"  å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")
print(f"  å±‚æ•°: {config.n_layer}")
print(f"  æ³¨æ„åŠ›å¤´æ•°: {config.n_head}")
print(f"  Embedding ç»´åº¦: {config.n_embd}")
print(f"  ä¸Šä¸‹æ–‡é•¿åº¦: {config.block_size} tokens")
print(f"  è¯è¡¨å¤§å°: {config.vocab_size:,}")

print(f"\nè®­ç»ƒé…ç½®:")
print(f"  è®­ç»ƒæ­¥æ•°: {config.max_iters}")
print(f"  æ‰¹å¤§å°: {config.batch_size}")
print(f"  åˆå§‹å­¦ä¹ ç‡: {config.learning_rate}")
print(f"  æœ€å°å­¦ä¹ ç‡: {config.min_lr}")
print(f"  Warmup æ­¥æ•°: {config.warmup_iters}")
print(f"  è®­ç»ƒæ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")

print(f"\næœ€ç»ˆæ€§èƒ½:")
final_losses = estimate_loss()
print(f"  è®­ç»ƒé›† Loss: {final_losses['train']:.4f}")
print(f"  éªŒè¯é›† Loss: {final_losses['val']:.4f}")

# è®¡ç®—å›°æƒ‘åº¦
train_perplexity = math.exp(final_losses['train'])
val_perplexity = math.exp(final_losses['val'])
print(f"  è®­ç»ƒé›†å›°æƒ‘åº¦: {train_perplexity:.2f}")
print(f"  éªŒè¯é›†å›°æƒ‘åº¦: {val_perplexity:.2f}")

print("\nğŸ‰ è¯çº§ GPT è®­ç»ƒå®Œæˆï¼")

print("\nç›¸æ¯”å­—ç¬¦çº§çš„æ”¹è¿›:")
print("  âœ“ æ›´é«˜æ•ˆçš„ç¼–ç ï¼ˆ~4x å‹ç¼©ï¼‰")
print("  âœ“ æ›´é•¿çš„æœ‰æ•ˆä¸Šä¸‹æ–‡")
print("  âœ“ æ›´å¥½çš„è¯­ä¹‰ç†è§£")
print("  âœ“ æ·»åŠ äº†å­¦ä¹ ç‡è°ƒåº¦")
print("  âœ“ æ·»åŠ äº†æ¢¯åº¦è£å‰ª")
print("  âœ“ æ·»åŠ äº† top-k é‡‡æ ·")
print("  âœ“ æ›´å¤§çš„æ¨¡å‹ï¼ˆ30M vs 10Mï¼‰")

print("\nä¸‹ä¸€æ­¥å»ºè®®:")
print("  1. æŸ¥çœ‹ training_curves.png äº†è§£è®­ç»ƒè¿‡ç¨‹")
print("  2. ç”¨ä¸åŒçš„æç¤ºè¯æµ‹è¯•ç”Ÿæˆè´¨é‡")
print("  3. å¦‚æœæ•ˆæœä¸ç†æƒ³ï¼Œå¯ä»¥:")
print("     - è®­ç»ƒæ›´å¤šæ­¥æ•°ï¼ˆ20000+ï¼‰")
print("     - åœ¨æ›´å¤§æ•°æ®é›†ä¸Šè®­ç»ƒï¼ˆWikiText-2, OpenWebTextï¼‰")
print("     - å¢å¤§æ¨¡å‹ï¼ˆ12 å±‚ï¼Œ768 ç»´ï¼‰")
print("  4. è¿›å…¥ä¸‹ä¸€é˜¶æ®µï¼šå¤š GPU è®­ç»ƒã€æ··åˆç²¾åº¦")